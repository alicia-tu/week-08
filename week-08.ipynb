{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Quantitative Textual Analysis - Week 8: Correlations and Classifications\n",
    "\n",
    "## Brezina 2018 ch. 5: Correlations\n",
    "\n",
    "### Pearson's correlation (pp. 142–146)\n",
    "\n",
    "Pearson's correlation (r) is expressed as follows:\n",
    "\n",
    "```math\n",
    "r = \\frac{\\text{covariance}}{SD_1 \\times SD_2}\n",
    "```\n",
    "\n",
    "Covariance, in turn, is expressed:\n",
    "\n",
    "```math\n",
    "\\text{covariance} = \\frac{\\text{sum of multiplied distances from } mean_1 \\text{ and } mean_2}{\\text{total no. of cases} - 1}\n",
    "```\n",
    "\n",
    "For example, suppose that we have five documents with $N_a$ adjectives and $N_n$ nouns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [(5, 10), (12, 15), (14, 25), (15, 26), (20, 30)]\n",
    "\n",
    "def covariance(corpus: list[(int, int)]):\n",
    "    mean_1 = sum([a for a, _ in corpus]) / len(corpus)\n",
    "    mean_2 = sum([b for _, b in corpus]) / len(corpus)\n",
    "\n",
    "    return sum([(mean_1 - a) * (mean_2 - b) for a, b in corpus]) / (len(corpus) - 1)\n",
    "\n",
    "docs_covariance = covariance(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the standard deviation for each variable (number of adjectives and number of nouns) using the **sample standard deviation** from Brezina 2018 p. 50:\n",
    "\n",
    "```math\n",
    "\\text{standard deviation}_\\text{sample} = \\sqrt{\\frac{\\text{sum of squared distances from the mean}}{\\text{total no. of cases - 1}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9384978052288936"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def sd_sample(arr: list[int]):\n",
    "    mean = sum(arr) / len(arr)\n",
    "\n",
    "    return math.sqrt(sum([(mean - x)**2 for x in arr]) / (len(arr) - 1))\n",
    "\n",
    "sd_1 = sd_sample([a for a, _ in docs])\n",
    "sd_2 = sd_sample([b for _, b in docs])\n",
    "\n",
    "docs_covariance / (sd_1 * sd_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, Pearson's correlation indicates a _very_ strong positive correlation in between the number of adjectives and the number of nouns in these (made-up) documents.\n",
    "\n",
    "Pearson's correlation will always range between -1 and +1: negative numbers indicate a negative correlation, and positive numbers indicate a positive correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to report with correlation measures\n",
    "\n",
    "As you can probably guess, it's important to report a p-value or confidence interval wiith your correlation statistics in order to give your readers a sense of statistical significance (which, as it turns out, is directly correlated to the number of observations).\n",
    "\n",
    "Note that the functions that we wrote don't care about the length of the input arrays, as long as their respective type signatures are obeyed (a list of 2-tuples of `int`s for `covariance` and a list of `int`s for `sd_sample`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using scipy instead of calculating Pearson's _r_ by hand\n",
    "\n",
    "Instead of calculating Pearson's _r_ and associated p-values by hand, we can use the `scipy` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=np.float64(0.9384978052288937), pvalue=np.float64(0.018139369943329754))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# zip(*docs) is a python-ism for \"split this list of tuples into separate tuples\"\n",
    "x, y = zip(*docs)\n",
    "\n",
    "stats.pearsonr(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Class Lab 1\n",
    "\n",
    "In the `treebanks/` directory, you'll find annotated versions of several of Sophocles' plays. (You don't need to know Greek to complete this exercise.)\n",
    "\n",
    "Each file contains sentence- and word-level annotations. We'll focus on the `word` elements, which look something like this:\n",
    "\n",
    "`<word id=\"12\" form=\"ἐκμάθοις\" lemma=\"ἐκμανθάνω\" postag=\"v2saoa---\" head=\"8\" relation=\"ATR\" cite=\"urn:cts:greekLit:tlg0011.tlg001:2\"/>`\n",
    "\n",
    "The `postag` (\"part-of-speech tag\") attribute will be our focus for this exercise. Its first letter tells us the part of speech for a given word --- `v` for \"verb,\" `a` for \"adjective,\" `n` for \"noun,\" and so on.\n",
    "\n",
    "Using the [`lxml`](https://lxml.de) library, write an xpath expression for parsing the `word` tags and their respective `@postag` attributes.\n",
    "\n",
    "Then determine if the correlation between the number of adjectives and number of nouns in a document that we see in the BNE corpus in Brezina 2018 also holds true for this subset of the Sophoclean corpus.\n",
    "\n",
    "Finally, run similar analyses for correlations between nouns and verbs and between verbs and adjectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12699/1226967945.py:17: FutureWarning: This search incorrectly ignores the root element, and will be fixed in a future version.  If you rely on the current behaviour, change it to './/word'\n",
      "  return tree.findall(\"//word\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'correl_adj_noun: PearsonRResult(statistic=np.float64(0.47723925112032944), pvalue=np.float64(0.1630785642475298)), correl_adj_verb: PearsonRResult(statistic=np.float64(0.9304682054809317), pvalue=np.float64(9.397365857894094e-05)), correl_noun_verb: PearsonRResult(statistic=np.float64(0.23544158280947364), pvalue=np.float64(0.5125839641001063))'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lxml import etree\n",
    "from scipy import stats\n",
    "\n",
    "import os\n",
    "\n",
    "directory = \"treebanks\"\n",
    "files = [\n",
    "    f\"{directory}/{f}\"\n",
    "    for f in os.listdir(directory)\n",
    "    if os.path.isfile(f\"{directory}/{f}\")\n",
    "]\n",
    "\n",
    "\n",
    "def get_words_from_treebank(f):\n",
    "    tree = etree.parse(f)\n",
    "\n",
    "    return tree.findall(\"//word\")\n",
    "\n",
    "\n",
    "def count_pos(words, pos: str):\n",
    "    return len([w for w in words if w.get(\"postag\", \" \")[0] == pos])\n",
    "\n",
    "\n",
    "docs = []\n",
    "\n",
    "for f in files:\n",
    "    words = get_words_from_treebank(f)\n",
    "    n_adj = count_pos(words, \"a\")\n",
    "    n_noun = count_pos(words, \"n\")\n",
    "    n_verb = count_pos(words, \"v\")\n",
    "\n",
    "    docs.append(dict(filename=f, n_adj=n_adj, n_noun=n_noun, n_verb=n_verb))\n",
    "\n",
    "correl_adj_noun = stats.pearsonr([d['n_adj'] for d in docs], [d['n_noun'] for d in docs])\n",
    "correl_adj_verb = stats.pearsonr([d['n_adj'] for d in docs], [d['n_verb'] for d in docs])\n",
    "correl_noun_verb = stats.pearsonr([d['n_noun'] for d in docs], [d['n_verb'] for d in docs])\n",
    "\n",
    "f\"correl_adj_noun: {correl_adj_noun}, correl_adj_verb: {correl_adj_verb}, correl_noun_verb: {correl_noun_verb}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we need significantly more data to reject the null hypothesis -- that is, to get the p-value for each correlation to a reasonably low number.\n",
    "\n",
    "Your task, then, is to find additional texts to supplement the texts that have been provided here. You might need to run POS-tagging on them using SpaCy or a similar library.\n",
    "\n",
    "Please be sure to include a justification of whatever texts you include as you build out this small \"corpus.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "### Factor Analysis\n",
    "\n",
    "Brezina 2018 (164) describes **factor analysis** as \"a complex mathematical procedure that reduces a large number of linguistic variables. This is done by considering correlations betwen variables...; those that correlate -- both positively and negatively -- are considered components of the same factor because they have a connection.... A **factor** is thus a group of related linguistic variables summarizing a more general tendency ... in the data.\"\n",
    "\n",
    "If you are interested in following up on factor analysis and using it for your final project, be sure to read over the rest of chapter 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with BERT\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
